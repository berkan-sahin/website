
#+title: Sorting Algorithms
#+date: 2022-03-23T12:15:48+03:00
#+draft: false
#+katex: true
#+options: tex: t
#+startup: latexpreview

* What is sorting
- Organize data into ascending/descending order
- Internal sort :: The data is always in the memory
  - *We will only analyze internal sort*
- External sort :: The data doesn't fit in the memory (e.g. a 30GB dataset sorted on a computer w/ 16GB of RAM)
  - Read a chunk from secondary storage, sort, write to disk, repeat until all data is sorted
  - Need to merge the sorted chunks in the disk
- Sorting can make some algos (like finding the intersection of 2 sets) faster (sorting may not be our ultimate goal)

** Efficiency of sorting
- Using $O(n \log{n})$ algorithms leads to sub-quadratic algos
  
  
  [[file:./img/log_table.png]]

** Applications of Sorting
- Closest pair :: Given n numbers, find the pair which are closest to each other
  - Once the numbers are sorted, do a linear scan ( $O(n)$ ), the closest elements are adjacent elems where the difference between them is minimum
- Element uniqueness :: Given a set of n items, are they all unique or are there any duplicates?
  - Once the numbers are sorted, do a linear scan and check if any two adjacent elements are equal
** Sorting Algorithms
*** Selection Sort $O(n^2)$
- List divided into sorted/unsorted
- find the largest item from unsorted part
- swap w/ the element at the end of unsorted part
- now the sorted part can grow by one
[[file:./img/selsort_visual.png]]

#+begin_src c
void swap(int *a, int *b) {
    int tmp = *a;
    *a = *b;
    *b = tmp;
}

int maxElem(int *arr, int size) {
    int maxIdx = 0; // Assume 1st elem is largest
    for (int i = 1; i < size; i++) {
        if (arr[i] > arr[maxIdx])
            maxIdx = i;
    }

    return maxIdx;
}

void selectionSort(int arr, int size) {
  for (int last = size - 1; last > 0; last--) {
    int largest = indexOfLargest(theArray, last + 1);
    swap(&arr[largest], &arr[last]);
  }
}
#+end_src

**** Analysis
- ~maxElem~ runs in $O(last + 1)$ time for each iteration
- ~swap~ runs in $O(1)$ time
- Overall time complexity: $\sum_{i = 1}^n i = O(n^2)$
- Total swaps: $n - 1$
- Total moves: $3 * (n - 1)$
- Best case = worst case = avg case = $O(n^2)$
- Selection sort only requires $O(n)$ moves
  - Useful when moves are *much* slower than comparisons
*** Insertion Sort $O(n^2)$
- List divided into sorted/unsorted
- The first element of the unsorted part is inserted in place in the sorted sublist
- At most $n - 1$ passes in a list of $n$ elements
[[file:./img/Insertionsort-before.png]]
[[file:./img/Insertionsort-after.png]]
**** Notes
- Items are sorted in place
- An incremental approach, which is useful for data coming in streams

**** C code
#+begin_src c
void insertionSort(int *arr, int n) {

  for (int boundary = 1; boundary < n; boundary++) {

    int item = arr[boundary];
    int targetIdx;

    // Shift elements until the desired position is found
    for (targetIdx = boundary; targetIdx > 0 && arr[targetIdx - 1] > item; targetIdx--)
        arr[targetIdx] = arr[targetIdx - 1];

    arr[targetIdx] = item;
  }
}
#+end_src

**** Analysis
***** Best case: $O(n)$
- The array is already sorted in ascending order
- The inner loop is skipped
- The number of move operations :: $2 * (n - 1) \implies O(n)$
- The number of comparisons :: $(n - 1) \implies O(n)$
***** Worst case: $O(n^2)$
- The array is sorted in reverse
- Inner loop is executed j times for $j = 1,2,3,...,n$
- The number of moves :: $2*(n-1) + \sum_{i=1}^{n-1}i = 2*(n-1)+\frac{n*(n-1)}{2} \implies O(n^2)$
***** Average case: $O(n^2)$
- *Needs probabilistic analysis*

*** Bubble Sort $O(n^2)$
- Divide the array into sorted/unsorted parts
- We assume a "bubble" which moves towards the end of the array
- Checks i-1 and ith element for $i = 2,3,4,...,k$, swaps them if ~A[i] < A[i-1]~
- where k denotes the imaginary boundary between the sorted and the unsorted sublists
- At each pass the largest element (in the unsorted portion) is moved to the end of the array, and k is decremented

**** C code
#+begin_src c
void bubbleSort(int *arr, int n) {
   char sorted = 0;

	for (int boundary = size; (boundary > 0) && !sorted; boundary--) {
      sorted = 1;
      for (int i = 0; i < boundary - 1; i++) {

         if (arr[i] > arr[i + 1]) {
            int tmp = arr[i];
            arr[i] = arr[i + 1];
            arr[i + 1] = tmp;
            sorted = 0;
         }
      }
   }
}
#+end_src

***** Notes
- Since bubble sort makes the array more ordered for each pass, it may reach the sorted state early, therefore it is a good idea to check if the array is sorted (to avoid unnecessary passes)

**** Analysis
***** Worst case: $O(n^2)$
- The array is in reverse order
- Therefore the algorithm always performs a swap and does n passes
- The number of moves :: $\sum_{i=1}^{n-1}3i = 3n(n-1)/2 \implies O(n^2)$
- The number of comparisons :: $\sum_{i=1}^{n-1}i \implies O(n^2)$
***** Best case: $O(n)$
- The array is already sorted
- Therefore the algorithm performs one pass and no swaps
- Number of moves :: $0 \implies O(1)$
- Number of comparisons :: $O(n)$
***** Average case: $O(n^2)$

*** Merge Sort $O(n \log{n})$
- A divide and conquer algorithm

**** Algorithm
1. Divide the array into two halves
2. Sort each half separately
3. Merge the two halves into one sorted array

**** Pseudocode
#+begin_src python
def merge_sort(Arr, begin, end):
    # the array is sorted (base case)
    if begin == end:
        return
    else:
        # sort halves independently
        mid = (p + r) / 2
        merge_sort(Arr, begin, mid)
        merge_sort(Arr, mid, end)
        # merge the sorted halves
        merge(Arr, begin, mid, end)
#+end_src
**** Merging two sorted sub-arrays
- Keep indices of the two subarrays (i,j)
- Compare A[i] and B[j]
- Move the smaller element to the result array
- increment the index of the arr containing the smaller element
- repeat until reaching the end of one of the arrays
- If one of the arrays has remaining items, move them to the result array
- Complexity :: $\Theta(n)$
**** C code
#+begin_src c
void merge(int *arr, int begin, int mid, int end) {
    int left = begin;
    int right = mid + 1;
    int totalSize = (end - begin + 1) * sizeof(int);
    int *tmpArr = (int*) malloc(totalSize);
    int tmpIndex = 0;

    while ((left <= mid) && (right <= end)) {
        if (arr[left] < arr[right]) {
            tmpArr[tmpIndex] = arr[left];
            left++;
        } else {
            tmpArr[tmpIndex] = arr[right];
            right++;
        }

        tmpIndex++;
    }

    // Look for any leftovers
    while (left <= mid) {
        tmpArr[tmpIndex] = arr[left];
        left++; tmpIndex++;
    }

    while (right <= end) {
        tmpArr[tmpIndex] = arr[right];
        right++; tmpIndex++;
    }

    // Move data back to the original array (memcpy is in string.h)
    memcpy(&arr[begin], tmpArr, totalSize);
    free(tmpArr);

}
void mergeSort_r(int *arr, int begin, int end) {
    if (begin < end) {
        int mid = (begin + end) / 2;
        mergeSort_r(arr, begin, mid);
        mergeSort_r(arr, mid + 1, end);
        merge(arr, begin, mid, end);
    }
}

void mergeSort(int *arr, int n){
    mergeSort_r(arr, 0, n - 1);
}

#+end_src

**** Analysis
***** Merge
- Complexity is always $O(n)$
***** Merge sort
****** Recurrence relation
\begin{align}
T(n) & = 2T(n/2) + \Theta(n) \\
T(1) & = \Theta(1)
\end{align}
****** Repeated substitution
\begin{align}
T(n) & = 2T(n/2) + \Theta(n) \\
     & = 2[2T(n/4) + \Theta(n/2)] + \Theta(n) \\
     & = 2^2 T(n/2^2) + 2\Theta(n/2) + \Theta(n)
= 2^2 T(n/2^2) + 2\Theta(n) \\
     & = 2^kT(n/2^k) + k\Theta(n) \\
& \text{(when k = log2(n))} \\
& = n*\Theta(1) + \log_2{n}\Theta(n) \\
& = \Theta(n \log_2{n})
\end{align}
****** Notes
- Merge sort is an extremely efficient algorithm (worst and avg cases are $O(n \log{n})$)
- But it requires an extra array to use during merge
- The extra array is not needed w/ a linked list
  - But with a linked list, dividing the list requires a linear pass (which is $O(n)$)
*** Quick Sort $O(n \log{n})$
- Another divide-and-conquer algorithm
- Difference from merge sort :: Hard work is done before the recursive calls
**** Algorithm
1. Partition the array into two parts
   - Choose an element called the pivot (hoping it's close to the median of the array)
   - Elements with values < pivot go to the 1st part, values >= pivot go to the 2nd part
2. Sort the arrays independently
3. Combine (concatenate) the sorted parts
**** Partitioning the array
1. Select a pivot element and place it into the 1st location
2. 3 regions are considered during partitioning
   - $S_1$, where all elements are < pivot
   - $S_2$, where all elements are >= pivot
   - The unknown region, which contains elements not yet compared w/ pivot

3. Compare elements in unknown w/ the pivot
   - If element belongs in $S_2$, increment firstUnknown
   - If element belongs in $S_1$
     1. swap w/ the first item of $S_2$
     2. increment both lastS1 and firstUnknown (since we know the item we swapped the unknown with is in $S_2$)

4. Determine the index for the pivot and move it

5. Call quick sort on $S_1$ and $S_2$
   - Every element in $S_1$ is smaller than any element in $S_2$
   - that is, $a < b \forall (a, b) \in (S_1, S_2)$
**** TODO Code
#+begin_src c
// a naive implementation that always takes 1st element as pivot
void qSort_r(int *arr, int begin, int end) {
    if (begin < end) {
        const int pivot = arr[begin];
        int boundary = begin + 1;

        /* Regions:
         * begin: pivot
         * begin + 1 to boundary - 1: elements smaller than pivot
         * boundary to unkonwn - 1: elements >= pivot
         * the rest of the array: elements not yet compared
         */
        for (int unknown = begin + 1; unknown <= end; unknown++) {
            if (arr[unkonwn] < pivot) {

                // Avoid unnecessary swap
                if (boundary != unkown) {
                    int tmp = arr[unkonwn];
                    arr[unkonwn] = arr[boundary];
                    arr[boundary] = tmp;
                }

                boundary++;
            }
        }

        // Don't move pivot if all elements are >= pivot
        if (boundary - 1 != begin) {
            arr[begin] = arr[boundary - 1];
            arr[boundary - 1] = pivot;
        }

        qSort_r(arr, begin, boundary - 2); // Don't include pivot
        qSort_r(arr, boundary, end);
    }
}

void quickSort(int *arr, int size) {
    qSort_r(arr, 0, size - 1);
}
#+end_src
**** Analysis
***** Worst case
- When the 1st element is selected as the pivot and the list is already sorted
- The pivot divides the list into two sublists of size $n-1$ and 0
- The number of key comparisons
  $(n-1)+(n-2)+...+(1) = n^2/2-n/2 \implies O(n^2)$
- The number of swaps
  $(n-1)+(n-2)+...+(1)=n^2/2-n/2 \implies O(n^2)$
***** Average case
- $O(n*\log_2{n})$
***** Best case
- $O(n * \log_2{n})$
*** Notes
  - Quicksort is one of the fastest sorting algorithms *that uses comparisons*
  - Sorting algorithms using comparisons cannot be faster than $O(n * \log{n})$
  - Algorithms like radix sort, counting sort etc. don't use comparisons
  - [[https://www.youtube.com/watch?v=_KhZ7F-jOlI][Why sorting algorithms w/ comparisons can't be faster than O(nlogn) (YouTube Video)]]
