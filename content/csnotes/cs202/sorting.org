
#+title: "Searching and Sorting"
#+date: 2022-03-23T12:15:48+03:00
#+draft: true
#+katex: true
#+options: tex: t
#+startup: latexpreview

[[file:https://cs.bilkent.edu.tr/~adayanik/cs202/slides/L2_Sorting.pptx][Slideshow]]
* Problem of the day
True or false?
1. $2n^2 + 1 = O(n^2)$ : True
2. $\sqrt{n} = O(\log{n})$ : False
   Because $\sqrt{n}$ grows faster than $\log{n}$
3. $\log{n} = O(\sqrt{n})$ : True
   Because $\sqrt{n}$ grows faster than $\log{n}$
4. $n^2(1+\sqrt{n}) = O(n^2\log{n})$ : False
   $n^2\sqrt{n}$ dominates the left hand side
5. $3n^2 + \sqrt{n} = O(n^2)$ : True
6. $\sqrt{n}\log{n} = O(n)$ : True
   Replace $\log{n}$ w/ $O(\sqrt{n})$
7. $\log{n} = O(n^{-1/2})$ : False
   RHS is decreasing, so cannot be upper bound
* Searching
** Sequential Search
#+begin_src cpp
int sequentialSearch(const int a[], int item, int n){
    int i;
	for (i = 0; i < n && a[i] != item; i++);
	if (i == n)
		return –1;
	return i;
}
#+end_src
- Unsuccessful search :: $O(n)$
- Successful search
  - Best case :: Item is in the 1st location of the array
    $O(1)$
  - Worst case :: Item is in the last location of the array
    $O(n)$
  - Average case :: The number of key comparisons 1, 2, ..., n
    (assuming uniform distribution)

    \begin{equation*}
    \frac{\sum_{i = 1}^{n} i}{n} = \frac{(n^2+n)/2}{n} => O(n)
    \end{equation*}
** Binary Search
- We need to have the elements sorted in order to use bin. search
#+begin_src cpp
int binarySearch( int a[], int size, int x) {
   int low =0;
   int high = size –1;
   int mid; 	  // mid will be the index of
   		  	  // target when it’s found.
   while (low <= high) {
 	   mid = (low + high)/2;
	   if (a[mid] < x)
        low = mid + 1;
	   else if (a[mid] > x)
		   high  = mid – 1;
     else
		   return mid;
   }
   return –1;
}
#+end_src
- Runs in $O(\log{n})$ time, since the problem size ~high - low~ is halved every iteration
* Sorting
** What is sorting
- Organize data into ascending/descending order
- Internal sort :: The data is always in the memory
  - *We will only analyze internal sort*
- External sort :: The data doesn't fit in the memory (e.g. a 30GB dataset sorted on a computer w/ 16GB of RAM)
  - Read a chunk from secondary storage, sort, write to disk, repeat until all data is sorted
  - Need to merge the sorted chunks in the disk
- Sorting can make some algos (like finding the intersection of 2 sets) faster (sorting may not be our ultimate goal)

** Efficiency of sorting
- Using $O(n \log{n})$ algorithms leads to sub-quadratic algos
  #+call: shot()

  #+RESULTS:
  [[file:./pics/220215-0916-30.png]]

** Applications of Sorting
- Closest pair :: Given n numbers, find the pair which are closest to each other
  - Once the numbers are sorted, do a linear scan ( $O(n)$ ), the closest elements are adjacent elems where the difference between them is minimum
- Element uniqueness :: Given a set of n items, are they all unique or are there any duplicates?
  - Once the numbers are sorted, do a linear scan and check if any two adjacent elements are equal
** Sorting Algorithms
*** Selection Sort $O(n^2)$
- List divided into sorted/unsorted
- find the largest item from unsorted part
- swap w/ the element at the end of unsorted part
- now the sorted part can grow by one
#+call: shot()

#+RESULTS:
[[file:./pics/220215-0939-01.png]]

#+begin_src cpp
typedef type-of-array-item DataType;

void swap(DataType &a, DataType &b) {
    DataType tmp = a;
    a = b;
    b = tmp;
}

int indexOfLargest(DataType arr[], int n) {
    int maxIdx = 0; // Assume 1st elem is largest
    for (int i = 1; i < n; i++) {
        if (arr[i] > arr[maxIdx])
            maxIdx = i;
    }

    return maxIdx;
}

void selectionSort( DataType theArray[], int n) {
  for (int last = n-1; last >= 1; --last) {
    int largest = indexOfLargest(theArray, last+1);
    swap(theArray[largest], theArray[last]);
  }
}
#+end_src

**** Analysis
- ~indexOfLargest~ runs in $O(last + 1)$ time for each iteration
- ~swap~ runs in $O(1)$ time
- Overall time complexity: $\sum_{i = 1}^n i = O(n^2)$
- Total swaps: $n - 1$
- Total moves: $3 * (n - 1)$
- Best case = worst case = avg case = $O(n^2)$
- Selection sort only requires $O(n)$ moves
  - Useful when moves are *much* slower than comparisons
*** Insertion Sort $O(n^2)$
- List divided into sorted/unsorted
- The first element of the unsorted part is inserted in place in the sorted sublist
- At most $n - 1$ passes in a list of $n$ elements
  #+call: shot()

  #+RESULTS:
  [[file:./pics/220215-0955-16.png]]

**** Notes
- Items are sorted in place
- Incremental approach :: Useful for streams

**** C++ code
#+begin_src cpp
void insertionSort(DataType theArray[], int n) {

  for (int unsorted = 1; unsorted < n; ++unsorted) {

    DataType nextItem = theArray[unsorted];
    int loc = unsorted;

    for (  ;(loc > 0) && (theArray[loc-1] > nextItem); --loc)
       theArray[loc] = theArray[loc-1];

    theArray[loc] = nextItem;
  }
}
#+end_src

**** Analysis
***** Best case: $O(n)$
- The array is already sorted in ascending order
- The inner loop is skipped
- The number of move operations :: $2 * (n - 1) \implies O(n)$
- The number of comparisons :: $(n - 1) \implies O(n)$
***** Worst case: $O(n^2)$
- The array is sorted in reverse
- Inner loop is executed j times for $j = 1,2,3,...,n$
- The number of moves :: $2*(n-1) + \sum_{i=1}^{n-1}i = 2*(n-1)+\frac{n*(n-1)}{2} \implies O(n^2)$
***** Average case: $O(n^2)$
- *Needs probabilistic analysis*

*** Bubble Sort $O(n^2)$
- Divide the array into sorted/unsorted parts
- We assume a "bubble" which moves towards the end of the array
- Checks i-1 and ith element for $i = 2,3,4,...,k$, swaps them if ~A[i] < A[i-1]~
- where k denotes the imaginary boundary between the sorted and the unsorted sublists
- At each pass the largest element (in the unsorted portion) is moved to the end of the array, and k is decremented

**** C++ code
#+begin_src cpp
void bubbleSort( DataType theArray[], int n) {
   bool sorted = false;

	for (int pass = 1; (pass < n) && !sorted; ++pass) {
      sorted = true;
      for (int index = 0; index < n-pass; ++index) {
         int nextIndex = index + 1;
         if (theArray[index] > theArray[nextIndex]) {
            swap(theArray[index], theArray[nextIndex]);
            sorted = false; // signal exchange
         }
      }
   }
}
#+end_src

***** Notes
- Since bubble sort makes the array more ordered for each pass, it may reach the sorted state early, therefore it is a good idea to check if the array is sorted (to avoid unnecessary passes)

**** Analysis
***** Worst case: $O(n^2)$
- The array is in reverse order
- Therefore the algorithm always performs a swap and does n passes
- The number of moves :: $\sum_{i=1}^{n-1}3i = 3n(n-1)/2 \implies O(n^2)$
- The number of comparisons :: $\sum_{i=1}^{n-1}i \implies O(n^2)$
***** Best case: $O(n)$
- The array is already sorted
- Therefore the algorithm performs one pass and no swaps
- Number of moves :: $0 \implies O(1)$
- Number of comparisons :: $O(n)$
***** Average case: $O(n^2)$

*** Merge Sort $O(n \log{n})$
- A divide and conquer algorithm

**** Algorithm
1. Divide the array into two halves
2. Sort each half separately
3. Merge the two halves into one sorted array

#+call: shot()

#+RESULTS:
[[file:./pics/220217-1357-13.png]]

**** Pseudocode
#+begin_src python
def merge_sort(Arr, begin, end):
    # the array is sorted (base case)
    if begin == end:
        return
    else:
        # sort halves independently
        mid = (p + r) / 2
        merge_sort(Arr, begin, mid)
        merge_sort(Arr, mid, end)
        # merge the sorted halves
        merge(Arr, begin, mid, end)
#+end_src
**** Merging two sorted sub-arrays
- Keep indices of the two subarrays (i,j)
- Compare A[i] and B[j]
- Move the smaller element to the result array
- increment the index of the arr containing the smaller element
- repeat until reaching the end of one of the arrays
- If one of the arrays has remaining items, move them to the result array
- Complexity :: $\Theta(n)$
#+call: shot()

#+RESULTS:
[[file:./pics/220217-1410-37.png]]
**** C++ code
#+begin_src cpp
void mergesort( DataType theArray[], int first, int last) {

	if (first < last) {

      int mid = (first + last)/2; 	// index of midpoint

      mergesort(theArray, first, mid);

      mergesort(theArray, mid+1, last);

      // merge the two halves
      merge(theArray, first, mid, last);
   }
}  // end mergesort
#+end_src

#+begin_src cpp
void merge(DataType arr[], int first, int mid, int last) {
 	DataType tempArray[MAX_SIZE]; 	// temporary array

	 int first1 = first; 	// beginning of first subarray
   int last1 = mid; 		// end of first subarray
   int first2 = mid + 1;	// beginning of second subarray
   int last2 = last;		// end of second subarray
   int index = first1; // next available location in tempArray

   for ( ; (first1 <= last1) && (first2 <= last2); ++index) {
      if (theArray[first1] < theArray[first2]) {
         tempArray[index] = theArray[first1];
         ++first1;
      }
      else {
          tempArray[index] = theArray[first2];
          ++first2;
      }
   }
    // finish off the first subarray, if necessary
   for (; first1 <= last1; ++first1, ++index)
      tempArray[index] = theArray[first1];

   // finish off the second subarray, if necessary
   for (; first2 <= last2; ++first2, ++index)
      tempArray[index] = theArray[first2];

   // copy the result back into the original array
   for (index = first; index <= last; ++index)
      theArray[index] = tempArray[index];
}
#+end_src
**** Analysis
***** Merge
- Complexity is always $O(n)$
***** Merge sort
****** Recurrence relation
\begin{align}
T(n) & = 2T(n/2) + \Theta(n) \\
T(1) & = \Theta(1)
\end{align}
****** Repeated substitution
\begin{align}
T(n) & = 2T(n/2) + \Theta(n) \\
     & = 2[2T(n/4) + \Theta(n/2)] + \Theta(n) \\
     & = 2^2 T(n/2^2) + 2\Theta(n/2) + \Theta(n)
= 2^2 T(n/2^2) + 2\Theta(n) \\
     & = 2^kT(n/2^k) + k\Theta(n) \\
& \text{(when k = log2(n))} \\
& = n*\Theta(1) + \log_2{n}\Theta(n) \\
& = \Theta(n \log_2{n})
\end{align}
****** Notes
- Merge sort is an extremely efficient algorithm (worst and avg cases are $O(n \log{n})$)
- But it requires an extra array to use during merge
- The extra array is not needed w/ a linked list
  - But with a linked list, dividing the list requires a linear pass (which is $O(n)$)
*** Quick Sort $O(n \log{n})$
- Another divide-and-conquer algorithm
- Difference from merge sort :: Hard work is done before the recursive calls
**** Algorithm
1. Partition the array into two parts
   - Choose an element called the pivot (hoping it's close to the median of the array)
   - Elements with values < pivot go to the 1st part, values >= pivot go to the 2nd part
2. Sort the arrays independently
3. Combine (concatenate) the sorted parts
**** Partitioning the array
#+call: shot()
1. Select a pivot element and place it into the 1st location
2. 3 regions are considered during partitioning
   - $S_1$, where all elements are < pivot
   - $S_2$, where all elements are >= pivot
   - The unknown region, which contains elements not yet compared w/ pivot
   #+call: shot()

   #+RESULTS:
   [[file:./pics/220217-1516-32.png]]

3. Compare elements in unknown w/ the pivot
   - If element belongs in $S_2$, increment firstUnknown
   - If element belongs in $S_1$
     1. swap w/ the first item of $S_2$
     2. increment both lastS1 and firstUnknown (since we know the item we swapped the unknown with is in $S_2$)

4. Determine the index for the pivot and move it
#+RESULTS:
[[file:./pics/220217-1500-57.png]]

5. Call quick sort on $S_1$ and $S_2$
   - Every element in $S_1$ is smaller than any element in $S_2$
   - that is, $a < b \forall (a, b) \in (S_1, S_2)$
**** TODO C++ code
**** Analysis
***** Worst case
- When the 1st element is selected as the pivot and the list is already sorted
- The pivot divides the list into two sublists of size $n-1$ and 0
- The number of key comparisons
  $(n-1)+(n-2)+...+(1) = n^2/2-n/2 \implies O(n^2)$
- The number of swaps
  $(n-1)+(n-2)+...+(1)=n^2/2-n/2 \implies O(n^2)$
***** Average case
- $O(n*\log_2{n})$
***** Best case
- $O(n * \log_2{n})$
*** Notes
  - Quicksort is one of the fastest sorting algorithms *that uses comparisons*
  - Sorting algorithms using comparisons cannot be faster than $O(n * \log{n})$
  - Algorithms like radix sort, counting sort etc. don't use comparisons
  - [[https://www.youtube.com/watch?v=_KhZ7F-jOlI][Why sorting algorithms w/ comparisons can't be faster than O(nlogn) (YouTube Video)]]
